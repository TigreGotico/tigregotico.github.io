<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TigreGotico Blog</title>
    <link>https://tigregotico.github.io</link>
    <atom:link href="https://tigregotico.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <description>Insights, tutorials, and thoughts on modern web development, technology trends, and best practices.</description>
    <language>en-us</language>
    <copyright>Copyright ¬© 2025 TigreGotico. All rights reserved.</copyright>
    <lastBuildDate>Mon, 24 Nov 2025 18:05:23 GMT</lastBuildDate>
    <pubDate>Wed, 26 Nov 2025 00:00:00 GMT</pubDate>

    <item>
      <title>OVOS &amp; HiveMind in the Manufacturing Industry</title>
      <link>https://tigregotico.github.io/blog/2025-11-26-OVOS-hivemind-industry</link>
      <guid isPermaLink="true">https://tigregotico.github.io/blog/2025-11-26-OVOS-hivemind-industry</guid>
      <pubDate>Wed, 26 Nov 2025 00:00:00 GMT</pubDate>
      <author>Casimiro Ferreira</author>
      <description>The COALA and WASABI EU projects have built an entire industrial voice-assistant framework around OVOS + HiveMind, integrating them with their own tools, UI, and conversation engines.</description>
      <content:encoded><![CDATA[
OVOS & HiveMind in the Manufacturing Industry

As the lead developer of OpenVoiceOS, maintained by a non-profit, and the creator of HiveMind, I‚Äôve always believed in open, privacy-respecting voice technology. What I did not anticipate was how quickly these tools would end up in industrial research, especially without any direct involvement from me.

The COALA and WASABI EU projects have built an entire industrial voice-assistant framework around OVOS + HiveMind, integrating them with their own tools, UI, and conversation engines.

I am not involved with these deployments, but the fact that the stack is being adopted organically is a strong validation of its design.

WASABI Open Call

The 2nd WASABI Open Call to provide financial support to at least 10 experiments led by SMEs recently closed.
This open call is designed to support AI-based digital assistance experiments involving SMEs from manufacturing.

All WASABI Open Call experiments are required to:

 run the WASABI/COALA OVOS Docker stack
 connect via HiveMind
 develop a custom OVOS Skill containing their industrial logic

The usage of OVOS/Hivemind is explained in these 2 documents from the Wasabi project:
Deliverable D2.1
Deliverable D2.4

!wasabiovos

Examples of Industrial Applications
Worker Guidance & Assembly Support

Experiments like TICONAI and SKITE are using OVOS skills to guide workers during complex tasks such as assembling components, validating procedures, or providing step-by-step instructions hands-free.
Quality Control and Error Reduction

Projects like WALLABI and HUMANENERDIA focus on providing workers with real-time instructions and checklists to prevent mistakes. Voice assistants help operators verify settings, remember safety checks, or cross-check parameters.
Predictive Maintenance Assistance

Experiments such as GENIUS-PM use the assistant to give maintenance techs quick access to machine health data, fault explanations, and repair steps‚Äîespecially when their hands are occupied.
Logistics, Material Handling & Warehouse Support

VELO and AIVEA use voice to help workers locate items, confirm inventory, or check delivery tasks while moving around a shop floor.
Onboarding and Training

ONBOARD and AI-MODE test how new employees can be guided through tasks using voice guidance, reducing the burden on supervisors.
Sustainability, Waste Tracking & Resource Efficiency

VAFER integrates voice interfaces with systems that monitor recycling, material reuse, and resource flows‚Äîhands-free reporting in factory environments.

All of these rely on OVOS and on HiveMind for routing communication between devices, Android UI, and backend systems.

What COALA/WASABI Built on Top of OVOS

Although the projects produced no open-source industrial skills, they did create several components around OVOS + HiveMind:
A RASA-based Domain Assistant (DA)

Earlier COALA research developed a RASA NLP pipeline trained on manufacturing conversations (about quality checks, troubleshooting, machine operation).
In WASABI, this RASA engine is plugged into OVOS as a skill, handling domain-specific dialog.
The COALA Android App

An Android front-end for workers, connecting to OVOS through HiveMind.

Early version released here:
https://github.com/BIBA-GmbH/Mycroft-Android

Features include:

 login via Keycloak
 text or voice chat
 UI for instructions, warnings, and notes
 HiveMind-based messaging
A Full Docker-Based Industrial Stack

Both projects ship a preconfigured Docker environment bundling:

 OVOS
 HiveMind
 Keycloak (user management)
 RASA NLP engine
 COALA connector services

This forms the standard industrial voice-assistant stack that all WASABI experiments must deploy.
An Industrial Speech Dataset

COALA published a multilingual speech dataset recorded in factories and workshops:
https://zenodo.org/record/8268928

Why Industry Chooses OVOS + HiveMind

The appeal is straightforward:

 Full transparency (crucial for regulated sectors)
 Local/edge deployment (no cloud dependency)
 Easy to integrate into existing equipment
 Modular enough for custom proprietary skills
Distributed voice networks (HiveMind satellites across a factory)

In short: the combination is flexible, vendor-neutral, and respects industrial data constraints.

Closing Thoughts

I didn‚Äôt set out to build an industrial standard.
I set out to build something open, reliable, and user-controlled.

Seeing OVOS and HiveMind adopted by COALA/WASABI, without my involvement or promotion, is a quiet but powerful sign that open-source voice technology is maturing.

A transparent, modular voice stack is no longer just a community dream.

It‚Äôs becoming part of the industrial toolset used to guide workers, reduce errors, improve maintenance, and ensure safer operations.

This is only the beginning.

---
Read more: <a href="https://tigregotico.github.io/blog/2025-11-26-OVOS-hivemind-industry">View full article</a>
]]></content:encoded>
    </item>

    <item>
      <title>What is OpenVoiceOS? A Beginner&apos;s Guide to Privacy-First Voice AI</title>
      <link>https://tigregotico.github.io/blog/2025-11-20-intro-to-ovos</link>
      <guid isPermaLink="true">https://tigregotico.github.io/blog/2025-11-20-intro-to-ovos</guid>
      <pubDate>Thu, 20 Nov 2025 00:00:00 GMT</pubDate>
      <author>TigreGotico</author>
      <description>Discover OpenVoiceOS, a Free and Open Source Software platform for voice AI that prioritizes privacy, transparency, and user control over your data.</description>
      <content:encoded><![CDATA[
What is OpenVoiceOS? A Beginner's Guide to Privacy-First Voice AI

In a world where most voice assistants like Alexa or Google Assistant are tied to big tech companies, OpenVoiceOS (OVOS) offers a different path. It's a Free and Open Source Software (FOSS) platform for voice AI.

Think of it this way: üó£Ô∏è OpenVoiceOS is an operating system and a framework, a complete toolkit for building voice-controlled devices. Unlike proprietary systems that send your voice commands to the cloud for processing, OVOS is designed to keep your data private and on your device. This means you have full control over your information, ensuring it's not being used for targeted ads or other commercial purposes without your consent.

The core idea is transparency and user control. You can see the code, modify it, and run it on your own hardware, from a Raspberry Pi to a desktop computer. This makes OVOS an ideal choice for developers, hobbyists, and businesses who want to build secure and privacy-respecting voice solutions.

TigreG√≥ticoLda is proud to be a key part of this mission. We are the lead developers of OpenVoiceOS. Note that while we provide professional services, OpenVoiceOS itself is a non-profit foundation based in the Netherlands, ensuring its core technology remains free and open for everyone.

What is HiveMind? Extending Your Voice Network

If OpenVoiceOS is the brain of your voice-controlled device, then HiveMind is the nervous system that connects everything.

HiveMind is a flexible communication protocol that allows a central OVOS instance (the "hub") to communicate with multiple other devices (the "satellites"). üì° It's designed to be lightweight and scalable. This means you can have one powerful machine running OpenVoiceOS, and then connect many other, less powerful devices to it for voice control.

For example, you could have a smart speaker with OVOS in your living room, and a small, inexpensive voice satellite in every other room. When you speak to a satellite, your command is securely sent to the main OVOS hub for processing, and the response is sent back to the satellite. This allows you to have a connected, multi-room voice network without needing a powerful, data-hungry device in every single location.

Together, OpenVoiceOS and HiveMind form a powerful, private, and customizable ecosystem for voice technology. As the lead developers of both projects, we have a unique and unparalleled understanding of the technology, which we bring to all our consulting and development services at TigreG√≥ticoLda.

---
Read more: <a href="https://tigregotico.github.io/blog/2025-11-20-intro-to-ovos">View full article</a>
]]></content:encoded>
    </item>

    <item>
      <title>Introducing phoonnx: The Next Generation of Open Voice for OpenVoiceOS</title>
      <link>https://tigregotico.github.io/blog/2025-10-06-phoonnx</link>
      <guid isPermaLink="true">https://tigregotico.github.io/blog/2025-10-06-phoonnx</guid>
      <pubDate>Mon, 06 Oct 2025 00:00:00 GMT</pubDate>
      <author>Unknown</author>
      <description>Today marks a significant step forward in the OpenVoiceOS journey with the official integration and adoption of phoonnx as our primary Text-to-Speech (TTS) engine.</description>
      <content:encoded><![CDATA[
Introducing phoonnx: The Next Generation of Open Voice for OpenVoiceOS

> This blog was originally posted in the OpenVoiceOS blog

Today marks a significant step forward in the OpenVoiceOS journey with the official adoption of phoonnx as our primary Text-to-Speech (TTS) framework. 

This new generation of voices is not just about quality; it's about consistency, efficiency, and fulfilling our mission for truly open, offline-ready voice assistants across the globe.

New Language: Introducing Basque!

Building on our previous work on Making Synthetic Voices From Scratch and our successful Arabic TTS Collaboration, we are excited to announce a new milestone in our language support: the addition of new voices for Basque (eu-ES)!

This includes both the male voice (Miro) and female voice (Dii), furthering our mission to support even low-resource languages that lack open, high-quality TTS options.

Previously only a robotic female voice was available, via the AhoTTS plugin made in collaboration with ILENIA.

Hear the results: Examples of the new Basque Open-Source Voices.

Miro (Male Voice):
Listen to Miro

Dii (Female Voice):
Listen to Dii

A Unified Voice for a Global Brand

As OpenVoiceOS expands to more languages and more devices, a crucial need has emerged: a cohesive brand identity conveyed through voice. 
We need a core set of voices, a standard male and female persona, that sounds the same, professional, and recognizable no matter where you are in the world or which language you are speaking.

This consistency is vital. Imagine installing OpenVoiceOS in Lisbon, Berlin, or Seattle, the voice should be instantly familiar. 
This is the power of a unified voice, creating a seamless and trustworthy user experience globally.

We are proud to share that TigreGotico has been instrumental in making this vision a reality. 
They are not only developing the core phoonnx engine but are also actively contributing to open datasets and training the default, multi-lingual OVOS voices. 
This internal collaboration accelerates development and ensures our voices are aligned with the open-source spirit of our platform.

The phoonnx Advantage: A Flexible TTS Ecosystem

phoonnx is more than just an inference tool; it is a complete training and inference framework built on the robust VITS architecture. 
This dual capability allows us to rapidly prototype, train, and deploy high-quality voices.

A key to this flexibility is the ability to support diverse phonemizers. 
A phonemizer (or G2P - Grapheme-to-Phoneme model) converts written text into the sequence of sound units (phonemes) the TTS model speaks. 
Different languages may require different, specialized phonemizers for accurate speech.

   eSpeak Compatibility: A core feature is that phoonnx models are fully compatible with the popular Piper TTS engine's runtime, provided they were trained using the widely available eSpeak phonemizer. This ensures easy deployment within the existing OVOS ecosystem and third party projects like Home Assistant.
   Custom Phonemizer Support: The framework is not limited to eSpeak. For example, we are excited to note that the high-quality Galician models developed by Proxecto N√≥s using the Cotovia phonemizer are fully compatible and can be used with the phoonnx pipeline.

This flexibility allows us to integrate and benefit from the work of other open-source projects. In fact, for inference, phoonnx can successfully use models originally trained by other projects, including Coqui, Mimic3, and Piper, solidifying its role as a universal TTS deployment tool.

Teasing the Future: Next-Gen G2P Models

Looking ahead, we are constantly working to improve G2P accuracy, especially for low-resource languages. We are currently developing and testing next-generation G2P models based on the powerful ByT5 architecture. These transformer-based models promise to deliver more accurate and robust phonemization across a wider range of languages.

You can follow their development here: G2P Models Collection.

In the near future a dedicated OVOS TTS plugin will be created for phoonnx and made the default for OpenVoiceOS, replacing the previous plugins: ovos-tts-plugin-piper and ovos-tts-plugin-nos. 

In the meantime you can try the new voices via the existing plugin ovos-tts-plugin-piper

All you need to do is pass the model urls under mycroft.conf

Progress Report: Available Languages

The collective work of the OpenVoiceOS and TigreGotico teams has resulted in a rapidly expanding library of open-source TTS models.

Currently Supported Languages:

 Arabic
 Basque
 Dutch
 English (US/GB)
 French
 German
 Italian
 Portuguese (Brazil/Portugal)
Spanish

Get Involved and Find the Models

We invite the community to explore and utilize these new resources. Your feedback is crucial to improving voice quality and expanding language coverage.

| Resource                 | Description                                                             | Link                                                                                                             |
|:-------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|
| Phoonnx Models       | The new phoonnx-trained TTS models in ONNX format.                      | phoonnx-tts-models |
| Piper/Phoonnx Voices | The full collection of OpenVoiceOS voices compatible with Piper.             | pipertts-voices       |
| Open Datasets        | Datasets used for training these voices, furthering open-data research. | tts-datasets             |

Help Us Build Voice for Everyone

OpenVoiceOS is more than software, it‚Äôs a mission. If you believe voice assistants should be open, inclusive, and user-controlled, here‚Äôs how you can help:
üí∏ Donate: Help us fund development, infrastructure, and legal protection.
üì£ Contribute Open Data: Share voice samples and transcriptions under open licenses.
üåç Translate: Help make OVOS accessible in every language.

We're not building this for profit. We're building it for people. With your support, we can keep voice tech transparent, private, and community-owned.

üëâ Support the project here

---
Read more: <a href="https://tigregotico.github.io/blog/2025-10-06-phoonnx">View full article</a>
]]></content:encoded>
    </item>

    <item>
      <title>OpenVoiceOS and Home Assistant: A Voice Automation Dream Team</title>
      <link>https://tigregotico.github.io/blog/2025-09-17-ovos_ha_dream_team</link>
      <guid isPermaLink="true">https://tigregotico.github.io/blog/2025-09-17-ovos_ha_dream_team</guid>
      <pubDate>Wed, 17 Sep 2025 00:00:00 GMT</pubDate>
      <author>Unknown</author>
      <description>In the world of open-source smart homes, some things just click. When you let Home Assistant handle the automation and let OVOS (Open Voice OS) handle the voice, you get a powerful partnership where each project shines. It‚Äôs a perfect synergy</description>
      <content:encoded><![CDATA[
OpenVoiceOS and Home Assistant: A Voice Automation Dream Team

> This blog was originally posted in the OpenVoiceOS blog

In the world of open-source smart homes, some things just click. When you let Home Assistant handle the automation and let OVOS (Open Voice OS) handle the voice, you get a powerful partnership where each project shines. It‚Äôs a perfect synergy: one is the undisputed champion of home automation, and the other is a flexible, private powerhouse for voice interaction.

Home Assistant excels at orchestrating your devices and routines, while OVOS provides unparalleled flexibility and privacy in voice interactions. Together, they create a system that's not only robust but also truly yours.

Let's explore how you can bring these two together to create a truly magical smart home experience.

Give Home Assistant an OVOS-Powered Voice

The most direct way to get started is to enhance Home Assistant's built-in voice capabilities with the specialized tools from the OVOS ecosystem. Our main goal is to make OVOS's powerful tools accessible to as many people as possible. 

To achieve this, we've developed dedicated Wyoming integrations that act as bridges, allowing any OVOS Text-to-Speech (TTS), Speech-to-Text (STT), or Wakeword plugin to be exposed to Home Assistant. 

This means you're not limited to a few options; you gain immediate access to the entire rich ecosystem of OVOS voice plugins, bringing a vast array of languages, voices, and recognition models directly into your Home Assistant setup.

 Wyoming OVOS STT: Convert spoken commands into text for Home Assistant to understand.
 Wyoming OVOS TTS: Enable Home Assistant to speak responses using OVOS's diverse voice options.
 Wyoming OVOS Wakeword: Integrate custom wakewords, allowing your Home Assistant setup to respond only when it hears your chosen trigger phrase.

The OVOS Wyoming Docker project makes getting these services up and running a breeze.

Plugin Highlights: Multi-language TTS powered by ILENIA

For us, accessibility is key. That includes language accessibility. We're proud that this integration allows us to bring high-quality, publicly funded voices from projects like ILENIA to a wider audience. Now, Home Assistant users can easily access fantastic, natural-sounding voices for languages like Catalan and Galician.

 Matxa TTS for Catalan: The ovos-tts-plugin-matxa-multispeaker-cat provides multi-speaker text-to-speech capabilities for the Catalan language.
 NosTTS for Galician: The ovos-tts-plugin-nos offers robust text-to-speech in Galician.

It‚Äôs a great example of how open collaboration benefits everyone.

!ILENIA logo

Setting up Wyoming Services in Home Assistant:

When configuring Wyoming services in Home Assistant, you'll typically refer to the official Home Assistant documentation. This process usually involves simply entering the IP address of your Docker container (or the host running your OVOS Wyoming services) into the Home Assistant web interface.

!wyoming setup in Home Assistant

!wyoming entities in Home Assistant

Let OVOS Be the Brains of the Conversation

Want to take it a step further? You can set up OVOS as a full-fledged conversational agent for Home Assistant using the Ollama integration.

!ollama setup in Home Assistant

In this setup, Home Assistant passes the user's text to the ovos-persona-server. OVOS then figures out what you want and tells Home Assistant what to answer. It‚Äôs like hiring a brilliant conversationalist to augment your smart home interactions.

Here‚Äôs the cool part: because ovos-persona-server uses Ollama-compatible endpoints, you can connect it to any app that supports the Ollama or OpenAI APIs. The possibilities are huge!

!chat with OVOS in Home Assistant

OVOS with the Voice Pe

Everyone is talking about Home Assistant Voice Preview Edition, a dedicated hardware device for voice control. If you own one, you can now easily integrate it with everything discussed so far.

!Configuring Home Assistant Voice Preview Edition

Welcome Your OVOS Devices into Home Assistant with HiveMind

If you have dedicated OVOS devices, the HiveMind HomeAssistant project is where the real magic happens. This integration makes your OVOS devices show up as native entities in Home Assistant, giving you a beautiful, unified control panel.

Setting up HiveMind Integration:

To integrate your OVOS devices via HiveMind, you'll typically add the HiveMind integration in Home Assistant. This involves providing connection details such as a name for the integration, an accesskey, password, siteid, host (IP address or hostname of your HiveMind server), and the port (defaulting to 5678). You may also have options to allowselfsigned certificates or enable legacyaudio depending on your setup.

!HiveMind setup in Home Assistant

Exposed Controls for OVOS Devices:

Once integrated, HiveMind exposes a comprehensive set of controls for your OVOS devices directly within Home Assistant. This allows you to manage various aspects of your OVOS device from the Home Assistant UI, including:

   Changing the Listening Mode (e.g., wakeword, always listening)
   Microphone Mute toggle
   OCP Player status and controls
   Actions like Reboot Device, Restart OVOS, and Shutdown Device
   Toggling Sleep Mode and SSH Service
   Manually Start Listening or Stop listening
   Controlling volume level

!HiveMind entities in Home Assistant

Notifications Integration:

HiveMind also enables your OVOS devices to function as notification targets within Home Assistant. This means you can configure Home Assistant automations to send spoken notifications directly to your OVOS devices, allowing them to "speak" alerts, reminders, or any other information you configure. This is exposed as a "Speak" notifier entity in Home Assistant.

!HiveMind notify service in Home Assistant

Media Player and Music Assistant Integration:

A fantastic feature of HiveMind integration is that your OVOS devices will show up as standard media players within Home Assistant. This allows you to control media playback on your OVOS devices directly from Home Assistant's media player interface. Furthermore, this integration extends to services like Music Assistant, enabling you to stream music and other audio content from Music Assistant through your OVOS devices, making them a seamless part of your whole-home audio system.

!HiveMind player in Home Assistant

!HiveMind player in Music Assistant

Give OVOS the Keys to the Kingdom

Finally, with the fantastic Skill HomeAssistant from community member mikejgray, you can give OVOS direct control over Home Assistant.

Install this skill, and your OVOS device can now command your smart home. Just say, "Hey Mycroft, turn on the living room lights," and watch the magic happen. It‚Äôs the classic voice assistant experience, but fully private, customizable, and powered by two best-in-class open-source projects.

The Perfect Match

When you let OVOS do the talking and Home Assistant do the automating, you get the best of both worlds. It‚Äôs a flexible, powerful, and fun combination that lets you build a smart home that is truly your own.

A Note on This Early Preview Release:

We're incredibly excited to share this integration with you! Please keep in mind that this is an early preview release and is still under active development. While it's functional and powerful, it hasn't undergone extensive testing and may have rough edges. We're actively working to refine it, and your feedback is invaluable! 

If you encounter any pain points or have ideas for improvements, please consider opening an issue or, even better, a Pull Request on our GitHub repositories. Your contributions help us make this even better for everyone. Thanks for being an early adopter and helping us shape the future of open-source voice!

Help Us Build Voice for Everyone

OpenVoiceOS is more than software, it‚Äôs a mission. If you believe voice assistants should be open, inclusive, and user-controlled, here‚Äôs how you can help:
üí∏ Donate: Help us fund development, infrastructure, and legal protection.
üì£ Contribute Open Data: Share voice samples and transcriptions under open licenses.
üåç Translate: Help make OVOS accessible in every language.

We're not building this for profit. We're building it for people. With your support, we can keep voice tech transparent, private, and community-owned.

üëâ Support the project here

---
Read more: <a href="https://tigregotico.github.io/blog/2025-09-17-ovos_ha_dream_team">View full article</a>
]]></content:encoded>
    </item>

    <item>
      <title>Making Synthetic Voices From Scratch</title>
      <link>https://tigregotico.github.io/blog/2025-06-26-making-synthetic-voices-from-scratch</link>
      <guid isPermaLink="true">https://tigregotico.github.io/blog/2025-06-26-making-synthetic-voices-from-scratch</guid>
      <pubDate>Thu, 26 Jun 2025 00:00:00 GMT</pubDate>
      <author>Unknown</author>
      <description>Creating a voice for a text-to-speech system usually requires a real person to spend hours recording audio. That‚Äôs expensive, time-consuming, and in many languages or accents, the voices just don‚Äôt exist at all</description>
      <content:encoded><![CDATA[
Making Synthetic Voices From Scratch

> This blog was originally posted in the OpenVoiceOS blog

What‚Äôs the problem?

Creating a voice for a text-to-speech (TTS) system usually requires a real person to spend hours recording audio. That‚Äôs expensive, time-consuming, and in many languages or accents, the voices just don‚Äôt exist at all, especially for open-source or offline use.

What did we do?

We developed a technique that allows us to create synthetic voices completely from scratch, even if we don‚Äôt have recordings from a real person. These voices:

 Work offline, even on small devices like a Raspberry Pi,
 Can speak any language, if there‚Äôs a good donor system available,
 Are fully customizable in sound and tone.

How does it work?
Start with an existing voice - We use an existing TTS voice (from any source) to generate lots of fake speech and text pairs.
Transform it into a new voice - We apply a special voice conversion process to change the sound of the voice to something new, like a different gender, age, or accent.
Train a compact model - With this synthetic data, we train a new voice model that sounds natural, speaks fluently, and runs entirely offline.

Why is this special?

 We can create a new voice without needing anyone to record lines.
 The voices don‚Äôt rely on cloud services, they work 100% offline.
 Each voice can be customized to sound unique or to match a character, personality, or accent.

What about ethics?

We take voice rights seriously.

 If we‚Äôre using a real person‚Äôs voice, we always get clear permission.
 If no permission is available, we use public domain recordings or create original voices that don‚Äôt copy anyone.
Our process actually makes the voice less recognizable, which helps protect privacy and avoid impersonation risks.

Real-world example

We applied this method to European Portuguese, a language that had no good offline voice options. In a short time, we built 4 brand-new, high-quality voices, no recordings needed, and they all run on small local devices.

> üí° Did we mention OpenVoiceOS now has a huggingface account? find all our TTS voices and more at huggingface.co/OpenVoiceOS

In short:

> We‚Äôve found a way to build natural-sounding, offline-ready synthetic voices, without needing a real speaker. It‚Äôs fast, ethical, and opens the door for more voices in more languages, for everyone.

Help Us Build Voice for Everyone 

If you believe that voice assistants should be open, inclusive, and user-controlled, we invite you to support OVOS: 
üí∏ Donate: Your contributions help us pay for infrastructure, development, and legal protections. 
üì£ Contribute Open Data: Speech models need diverse, high-quality data. If you can share voice samples, transcripts, or datasets under open licenses, let's collaborate. 
üåç Help Translate: OVOS is global by nature. Translators make our platform accessible to more communities every day. 

We're not building this for profit. We're building it for people. And with your help, we can ensure open voice has a future‚Äîtransparent, private, and community-owned. 

üëâ Support the project here

---
Read more: <a href="https://tigregotico.github.io/blog/2025-06-26-making-synthetic-voices-from-scratch">View full article</a>
]]></content:encoded>
    </item>

    <item>
      <title>No Language Left Behind</title>
      <link>https://tigregotico.github.io/blog/2023-10-16-no-language-left-behind</link>
      <guid isPermaLink="true">https://tigregotico.github.io/blog/2023-10-16-no-language-left-behind</guid>
      <pubDate>Mon, 16 Oct 2023 00:00:00 GMT</pubDate>
      <author>Casimiro Ferreira</author>
      <description>Eliminating language barriers in OpenVoiceOS through language detection, translation plugins, and bidirectional translation capabilities.</description>
      <content:encoded><![CDATA[
No Language Left Behind

> This post was originally posted in my (now defunct) personal blog

In today's globally connected world, the myriad of languages we speak stands as a testament to our diverse cultures and rich communication. 

I am  committed to eliminating language barriers in OpenVoiceOS (OVOS), an open-source voice assistant platform, ensuring no one is excluded from meaningful conversations.

At its core, OVOS is more than just a voice assistant platform. It's a community-driven effort to make technology more accessible to people worldwide. 
OVOS aims to empower individuals to interact with technology in their native language, and that's where the new language plugins come into play.

Language Detection from Audio

To further embrace linguistic diversity, I have incorporated language detection from audio into OVOS. 
This enables OVOS to identify the language spoken in audio inputs. 
This capability is made possible through plugins such as the new FasterWhisperLangClassifier plugin.

OVOS uses a Speech-to-Text (STT) plugin to convert spoken language into text. 
The STT plugin is crucial for understanding and processing user input, and the information from language detection plays a significant role in this process. 
The STT plugin must be aware of the language being spoken to provide accurate transcriptions.

I created several plugins that provide this functionality:
ovos-audio-transformer-plugin-speechbrain-langdetect
ovos-audio-transformer-plugin-speechflow-langdetect
ovos-stt-plugin-fasterwhisper

NOTE: language detection from audio is constrained to the languages configured in OVOS config, this rejects all classifications for languages you know will never be spoken around your OVOS setup

Configuration

You can configure the language classifier plugins with different settings and models, depending on your requirements. 

For example, to use a specific model in FasterWhisperLangClassifier:

Text Language Translation

In addition to language detection from audio, I also implemented text language translation.

Translation plugins are the backbone of OVOS's multilingual capabilities for text handling.
It ensures that when you communicate with OVOS in a language not natively supported, your text is translated, and you receive responses in your preferred language. 

No Language Left Behind (NLLB) is a breakthrough project from Meta that gives the name to this blog post, NLLB is a open-source model capable of delivering high-quality translations directly between 200 languages‚Äîincluding low-resource languages like Asturian, Luganda, Urdu and more.

The NLLB plugin is the primary text language translation plugin that enables OVOS to handle languages that are not natively understood by the assistant. 
This is especially useful when users communicate in languages that are not part of the assistant's native language set. 
Skills are generally slow to get fully translated, a huge amount of work is needed to fully support a new language, now users no longer depend on explicit support for their language!

A remote version of this plugin also exists to allow usage of this functionality in less powerful hardware, a list of public servers is provided out of the box, but users are encouraged to self host

Warning there are associated risk with using a public server, read my previous blog post The Trust Factor in Public Servers

Key Highlights of translation plugins
On-Demand Translation: With this plugin, OVOS can translate text and utterances in real-time.
Extensive Language Support: It covers a wide range of languages, making it possible for OVOS to communicate effectively with users globally.
Easy Configuration: Setting up the plugin is straightforward, allowing OVOS developers to integrate it seamlessly.

Translation plugins of note:
ovos-translate-plugin-nllb
ovos-translate-server-plugin

Configuration

The OVOS Bidirectional Translation Plugin

The OVOS Bidirectional Translation plugin is a game-changer for OVOS's language capabilities. 
It includes two main components: the Utterance Translator and the Dialog Translator.
Together, they enable OVOS to understand and respond in virtually any language, breaking down the language barrier once and for all.

Key Highlights
Utterance Transformer: Translates incoming text and adjusts the language of the session temporarily, ensuring a seamless conversational experience.
Language Verification: This optional feature corrects the session language based on the detected text language, ideal for chat platforms with multilingual users.
Dialog Transformer: Translates the spoken dialog back to the original session language, enabling OVOS to provide responses in the original language.
Configurable: The Bidirectional Translation plugin can be customized to fit your specific requirements. You can enable or disable features like language verification and more.

Pre-Requisites

Before you can harness the power of the Bidirectional Translation Plugin, you need to have the following components in place:
Language Detection: If you enabled language verification, ensure that you have a good language detection module configured in OVOS. This is vital for the plugin to identify the language in which the text is provided.
Translation Module: OVOS relies on a translation plugin to convert text from one language to another. You can use either a local translation module like ovos-translate-plugin-nllb or a remote translation module like ovos-translate-server-plugin.

Configuration

Configuring the Bidirectional Translation plugin is straightforward. You can define your settings in the OVOS configuration file. Here's an example configuration for both the Utterance Transformer and the Dialog Transformer:

These configurations allow you to tailor the behavior of the Bidirectional Translation plugin to your specific requirements.

How It Works Together

All these components can work together or standalone, depending on your specific requirements:
Language Detection from Audio: This informs the STT plugin to transcribe spoken language accurately.
Utterance Translation: a translation plugin is used when the text is in a language not natively understood by the assistant, ensuring a seamless conversation.
Dialog Translation: even if the internally generated speech is in a different language, the translation plugin ensures output (TTS) in the original language

The Bidirectional Translation plugin comes into play when you need to communicate in your preferred language regardless of the language OVOS is configured in. It bridges the language gap and ensures that every user can comfortably communicate in their chosen language.

The OVOS Community

Beyond software, I see a vibrant community forming around our shared passion for making technology universally accessible. 
With the introduction of these new language plugins and the capability for language detection from audio, I'm driven by a strong commitment to inclusivity and linguistic diversity.

I extend a warm invitation to developers and language enthusiasts from around the world to join me in this mission. 
Whether your goal is to contribute to language support, translate skills, or explore new features, I wholeheartedly welcome you to join me in pursuing a world where no language is left behind.

---
Read more: <a href="https://tigregotico.github.io/blog/2023-10-16-no-language-left-behind">View full article</a>
]]></content:encoded>
    </item>
  </channel>
</rss>