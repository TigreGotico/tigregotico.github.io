{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ™ï¸ Hybrid Low-Resource TTS Dataset Synthesis Pipeline\n",
    "\n",
    "This notebook implements the whitepaper workflow: **Format Conversion -> Denoising -> Trimming -> Normalization -> WPM Filtering -> Voice Cloning**.\n",
    "\n",
    "### Usage Instructions:\n",
    "1. **Configure Variables:** Set your base paths in the first code cell.\n",
    "2. **Run Sequentially:** Execute cells in order. Dependencies are re-installed per step to avoid conflicts.\n",
    "3. **Check Outputs:** Each step will output to a subdirectory within your `WORK_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# âš™ï¸ GLOBAL CONFIGURATION (ENV VARS)\n",
    "# ==========================================\n",
    "\n",
    "# Base directory for processing\n",
    "os.environ[\"WORK_DIR\"] = \"/run/media/miro/endeavouros/processing_workspace\"\n",
    "\n",
    "# Input Data Sources\n",
    "os.environ[\"RAW_AUDIO_SOURCE\"] = \"/run/media/miro/endeavouros/TTS_datasets/raw_audio\"\n",
    "os.environ[\"METADATA_CSV\"] = \"/run/media/miro/endeavouros/TTS_datasets/metadata.csv\"\n",
    "os.environ[\"REF_VOICES_DIR\"] = \"/run/media/miro/endeavouros/ref_voices\"\n",
    "\n",
    "# Processing Parameters\n",
    "os.environ[\"TARGET_LUFS\"] = \"-20.0\"\n",
    "os.environ[\"WPM_STD_THRESHOLD\"] = \"2.0\"\n",
    "os.environ[\"MIN_TRIM_DURATION\"] = \"1.0\"\n",
    "os.environ[\"DEVICE\"] = \"cuda\" # or cpu, mps\n",
    "\n",
    "# Define pipeline subdirectories (Chain of custody)\n",
    "base = os.environ[\"WORK_DIR\"]\n",
    "os.environ[\"DIR_CONVERTED\"] = os.path.join(base, \"1_converted\")\n",
    "os.environ[\"DIR_DENOISED\"] = os.path.join(base, \"2_denoised\")\n",
    "os.environ[\"DIR_TRIMMED\"] = os.path.join(base, \"3_trimmed\")\n",
    "os.environ[\"DIR_NORMALIZED\"] = os.path.join(base, \"4_normalized\")\n",
    "os.environ[\"DIR_FILTERED\"] = os.path.join(base, \"5_wpm_filtered\")\n",
    "os.environ[\"DIR_FINAL_VC\"] = os.path.join(base, \"6_vc_output\")\n",
    "\n",
    "print(f\"Work Directory: {os.environ['WORK_DIR']}\")\n",
    "print(f\"Target Device: {os.environ['DEVICE']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Step 1: Format Standardization\n",
    "Converts mixed audio formats (mp3, ogg, flv, etc.) to standard WAV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstall specific dependencies for this step\n",
    "!pip install pydub -q\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pydub import AudioSegment\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = os.environ.get(\"RAW_AUDIO_SOURCE\")\n",
    "OUTPUT_DIR = os.environ.get(\"DIR_CONVERTED\")\n",
    "\n",
    "SUPPORTED_FORMATS = {'.mp3', '.mp4', '.wma', '.ogg', '.flv', '.aac', '.m4a', '.amr', '.wav'}\n",
    "\n",
    "def convert_dataset(input_path, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    print(f\"Converting from {input_path} -> {output_path}\")\n",
    "    \n",
    "    for root, _, files in os.walk(input_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            base, ext = os.path.splitext(filename)\n",
    "            \n",
    "            if ext.lower() not in SUPPORTED_FORMATS:\n",
    "                continue\n",
    "\n",
    "            target_file = os.path.join(output_path, f\"{base}.wav\")\n",
    "            \n",
    "            # If already wav, just copy to standardize location, else convert\n",
    "            if ext.lower() == '.wav':\n",
    "                shutil.copy2(file_path, target_file)\n",
    "            else:\n",
    "                try:\n",
    "                    audio = AudioSegment.from_file(file_path)\n",
    "                    audio.export(target_file, format='wav')\n",
    "                    print(f\"Converted: {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to convert {filename}: {e}\")\n",
    "\n",
    "convert_dataset(INPUT_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Audio Denoising\n",
    "Uses `resemble-enhance` to remove background noise and enhance speech quality.\n",
    "*Note: This step can be resource intensive.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstall specific dependencies for this step\n",
    "!pip install resemble-enhance torchaudio --upgrade -q\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from resemble_enhance.enhancer.inference import denoise\n",
    "\n",
    "INPUT_DIR = os.environ.get(\"DIR_CONVERTED\")\n",
    "OUTPUT_DIR = os.environ.get(\"DIR_DENOISED\")\n",
    "DEVICE = os.environ.get(\"DEVICE\", \"cpu\")\n",
    "\n",
    "def process_denoising(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith(\".wav\")]\n",
    "    \n",
    "    for fname in tqdm(files, desc=\"Denoising Audio\"):\n",
    "        in_path = os.path.join(input_dir, fname)\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "        \n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            dwav, sr = torchaudio.load(in_path)\n",
    "            # Mono mix if necessary\n",
    "            if dwav.shape[0] > 1:\n",
    "                dwav = dwav.mean(dim=0)\n",
    "            else:\n",
    "                dwav = dwav.squeeze(0)\n",
    "            \n",
    "            # Denoise\n",
    "            denoised_wav, new_sr = denoise(dwav, sr, DEVICE)\n",
    "            \n",
    "            # Save\n",
    "            torchaudio.save(out_path, denoised_wav.unsqueeze(0), new_sr)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fname}: {e}\")\n",
    "\n",
    "process_denoising(INPUT_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Silence Trimming\n",
    "Trims leading/trailing silence using `librosa`. Removes files that are too short after trimming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstall specific dependencies for this step\n",
    "!pip install librosa soundfile -q\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_DIR = os.environ.get(\"DIR_DENOISED\")\n",
    "OUTPUT_DIR = os.environ.get(\"DIR_TRIMMED\")\n",
    "MIN_DURATION = float(os.environ.get(\"MIN_TRIM_DURATION\", 1.0))\n",
    "\n",
    "def trim_silence(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = glob(os.path.join(input_dir, '*.wav'))\n",
    "    \n",
    "    for fpath in tqdm(files, desc=\"Trimming Silence\"):\n",
    "        fname = os.path.basename(fpath)\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "        \n",
    "        try:\n",
    "            # Load\n",
    "            audio, sr = librosa.load(fpath, sr=None, mono=True)\n",
    "            \n",
    "            # Trim\n",
    "            trimmed_audio, _ = librosa.effects.trim(audio, top_db=40)\n",
    "            duration = len(trimmed_audio) / sr\n",
    "            \n",
    "            # Filter short files\n",
    "            if duration >= MIN_DURATION:\n",
    "                sf.write(out_path, trimmed_audio, sr)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fname}: {e}\")\n",
    "\n",
    "trim_silence(INPUT_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Volume Normalization\n",
    "Normalizes audio to a target loudness (e.g., -20.0 LUFS/dBFS) to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstall specific dependencies for this step\n",
    "!pip install pydub -q\n",
    "\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_DIR = os.environ.get(\"DIR_TRIMMED\")\n",
    "OUTPUT_DIR = os.environ.get(\"DIR_NORMALIZED\")\n",
    "TARGET_DBFS = float(os.environ.get(\"TARGET_LUFS\", -20.0))\n",
    "\n",
    "def normalize_volume(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = glob(os.path.join(input_dir, '*.wav'))\n",
    "    \n",
    "    for fpath in tqdm(files, desc=\"Normalizing Volume\"):\n",
    "        fname = os.path.basename(fpath)\n",
    "        out_path = os.path.join(output_dir, fname)\n",
    "        \n",
    "        try:\n",
    "            audio = AudioSegment.from_wav(fpath)\n",
    "            current_dbfs = audio.dBFS\n",
    "            gain = TARGET_DBFS - current_dbfs\n",
    "            \n",
    "            normalized = audio.apply_gain(gain)\n",
    "            normalized.export(out_path, format=\"wav\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error normalizing {fname}: {e}\")\n",
    "\n",
    "normalize_volume(INPUT_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: WPM (Words Per Minute) Filtering\n",
    "Filters out outliers based on speaking rate using the provided transcript CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstall specific dependencies for this step\n",
    "!pip install pandas soundfile -q\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "INPUT_DIR = os.environ.get(\"DIR_NORMALIZED\")\n",
    "CSV_PATH = os.environ.get(\"METADATA_CSV\")\n",
    "OUTPUT_DIR = os.environ.get(\"DIR_FILTERED\")\n",
    "STD_THRESHOLD = float(os.environ.get(\"WPM_STD_THRESHOLD\", 2.0))\n",
    "\n",
    "def calculate_wpm(row, audio_dir):\n",
    "    file_path = os.path.join(audio_dir, str(row['file_name']))\n",
    "    # Handle missing extensions in CSV if necessary\n",
    "    if not file_path.endswith('.wav') and not os.path.exists(file_path):\n",
    "         file_path += \".wav\"\n",
    "         \n",
    "    cleaned_transcript = re.sub(r'[^\\w\\s]', '', str(row['transcript'])).strip().lower()\n",
    "    word_count = len(cleaned_transcript.split())\n",
    "\n",
    "    if word_count == 0 or not os.path.exists(file_path):\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        with sf.SoundFile(file_path) as f:\n",
    "            duration_min = (len(f) / f.samplerate) / 60.0\n",
    "        return word_count / duration_min if duration_min > 0 else 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def filter_wpm(input_dir, csv_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load CSV (Assuming Pipe separated based on provided script)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, sep='|', names=['file_name', 'transcript'], header=None)\n",
    "    except:\n",
    "        # Fallback for standard CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "    print(\"Calculating WPM...\")\n",
    "    df['WPM'] = df.apply(lambda row: calculate_wpm(row, input_dir), axis=1)\n",
    "    valid_data = df[df['WPM'] > 0]\n",
    "    \n",
    "    # Stats\n",
    "    mean_wpm = valid_data['WPM'].mean()\n",
    "    std_wpm = valid_data['WPM'].std()\n",
    "    lower = mean_wpm - (STD_THRESHOLD * std_wpm)\n",
    "    upper = mean_wpm + (STD_THRESHOLD * std_wpm)\n",
    "    \n",
    "    print(f\"Mean WPM: {mean_wpm:.2f} | Range: {lower:.2f} - {upper:.2f}\")\n",
    "    \n",
    "    filtered = valid_data[(valid_data['WPM'] >= lower) & (valid_data['WPM'] <= upper)]\n",
    "    \n",
    "    print(f\"Copying {len(filtered)} valid files...\")\n",
    "    new_meta = []\n",
    "    for _, row in filtered.iterrows():\n",
    "        fname = str(row['file_name'])\n",
    "        if not fname.endswith('.wav'): fname += \".wav\"\n",
    "        \n",
    "        src = os.path.join(input_dir, fname)\n",
    "        dst = os.path.join(output_dir, fname)\n",
    "        \n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            new_meta.append(f\"{fname}|{row['transcript']}\")\n",
    "            \n",
    "    # Save new metadata\n",
    "    with open(os.path.join(output_dir, \"metadata_filtered.csv\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(new_meta))\n",
    "\n",
    "filter_wpm(INPUT_DIR, CSV_PATH, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Voice Conversion (Synthesis)\n",
    "Generates the final synthetic dataset using `ChatterboxVC` and the reference voices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstall specific dependencies for this step\n",
    "# Note: Replace 'chatterbox' with specific git url if it is not on PyPI\n",
    "!pip install torch torchaudio tqdm chatterbox -q\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "from chatterbox.vc import ChatterboxVC\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_DIR = os.environ.get(\"DIR_FILTERED\")\n",
    "OUTPUT_BASE = os.environ.get(\"DIR_FINAL_VC\")\n",
    "REFS_FOLDER = os.environ.get(\"REF_VOICES_DIR\")\n",
    "DEVICE = os.environ.get(\"DEVICE\", \"cpu\")\n",
    "\n",
    "def run_voice_conversion():\n",
    "    print(f\"Loading Model on {DEVICE}...\")\n",
    "    try:\n",
    "        model = ChatterboxVC.from_pretrained(DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {e}\")\n",
    "        return\n",
    "\n",
    "    ref_voices = [f for f in os.listdir(REFS_FOLDER) if f.endswith('.wav')]\n",
    "    source_files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.wav')]\n",
    "\n",
    "    for ref_voice in tqdm(ref_voices, desc=\"Processing Ref Voices\"):\n",
    "        voice_id = os.path.splitext(ref_voice)[0]\n",
    "        ref_path = os.path.join(REFS_FOLDER, ref_voice)\n",
    "        \n",
    "        voice_out_dir = os.path.join(OUTPUT_BASE, voice_id)\n",
    "        os.makedirs(voice_out_dir, exist_ok=True)\n",
    "\n",
    "        for fname in tqdm(source_files, desc=f\"Converting to {voice_id}\", leave=False):\n",
    "            out_file = os.path.join(voice_out_dir, fname)\n",
    "            if os.path.exists(out_file):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                wav = model.generate(\n",
    "                    audio=os.path.join(INPUT_DIR, fname),\n",
    "                    target_voice_path=ref_path,\n",
    "                )\n",
    "                ta.save(out_file, wav, model.sr)\n",
    "            except Exception as e:\n",
    "                # Silent fail to keep loop moving\n",
    "                continue\n",
    "\n",
    "run_voice_conversion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
